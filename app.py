import os
import json
import sys
import subprocess
from datetime import datetime

import pandas as pd
import streamlit as st


# -------------------------------
# MUST be first Streamlit command
# -------------------------------
st.set_page_config(
    page_title="ä¸€é€±è¾²æ¥­æ°£è±¡é å ± + è¾²æ¥­ç©æº«åˆ†æï¼ˆé å ±è§£è®€ï¼‰",
    layout="wide",
)


# ===============================
# Settings
# ===============================
DATA_DIR = "weather_data"


# ===============================
# Helpers
# ===============================
def ensure_data_dir() -> None:
    os.makedirs(DATA_DIR, exist_ok=True)


def list_json_files():
    if not os.path.exists(DATA_DIR):
        return []
    files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(".json")]
    files.sort()
    return files


def load_latest_json():
    files = list_json_files()
    if not files:
        return None, None

    latest_file = files[-1]
    path = os.path.join(DATA_DIR, latest_file)
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f), latest_file
    except Exception as e:
        return {"_error": f"Failed to read {latest_file}: {e}"}, latest_file


def run_crawler():
    """
    Try to run crawler.py using current python.
    Works on Streamlit Cloud if crawler.py exists in repo and dependencies are installed.
    """
    if not os.path.exists("crawler.py"):
        return False, "æ‰¾ä¸åˆ° crawler.pyï¼ˆè«‹ç¢ºèª repo å…§æœ‰ crawler.pyï¼‰"

    ensure_data_dir()

    try:
        p = subprocess.run(
            [sys.executable, "crawler.py"],
            capture_output=True,
            text=True,
            check=False,
        )
        # crawler may print logs
        if p.returncode != 0:
            return False, f"crawler.py åŸ·è¡Œå¤±æ•—ï¼ˆexit={p.returncode}ï¼‰\n\nSTDERR:\n{p.stderr}\n\nSTDOUT:\n{p.stdout}"
        return True, f"crawler.py åŸ·è¡ŒæˆåŠŸ\n\nSTDOUT:\n{p.stdout}"
    except Exception as e:
        return False, f"ç„¡æ³•åŸ·è¡Œ crawler.pyï¼š{e}"


def extract_temps_from_json(data: dict):
    """
    Best-effort parser.
    Your CWA JSON schema may vary. If parsing fails, we fallback to a demo temps list.
    """
    # Fallback demo temps
    fallback = [18, 20, 22, 23, 21, 19, 18]

    if not isinstance(data, dict):
        return fallback

    # Common places we might find temps (schema-dependent)
    # If you later confirm your JSON structure, we can make this exact.
    # For now: try to locate any list of 7 numbers inside.
    def find_numbers(obj):
        nums = []
        if isinstance(obj, dict):
            for v in obj.values():
                nums.extend(find_numbers(v))
        elif isinstance(obj, list):
            for v in obj:
                nums.extend(find_numbers(v))
        else:
            # try parse numeric strings
            if isinstance(obj, (int, float)):
                nums.append(float(obj))
            elif isinstance(obj, str):
                try:
                    nums.append(float(obj))
                except Exception:
                    pass
        return nums

    nums = find_numbers(data)
    # heuristic: if we have >=7 numbers, take last 7 as "temps"
    if len(nums) >= 7:
        temps = [round(x, 1) for x in nums[-7:]]
        # sanity: avoid nonsense like huge ids, timestamps etc.
        # keep values within plausible temperature range
        temps2 = [t for t in temps if -10 <= t <= 45]
        if len(temps2) >= 7:
            return temps2[-7:]
    return fallback


def crop_range(crop: str):
    table = {
        "æ°´ç¨»": (20, 30),
        "ç‰ç±³": (18, 30),
        "é«˜éº—èœ": (15, 25),
        "ç•ªèŒ„": (18, 28),
    }
    return table.get(crop, (18, 28))


def impact_judgement(avg_temp: float, opt_min: float, opt_max: float):
    if avg_temp < opt_min:
        return "åä½", "âš ï¸", "æ°£æº«åä½ï¼Œä½œç‰©ç”Ÿé•·é€Ÿç‡å¯èƒ½æ”¾ç·©ï¼Œéœ€ç•™æ„ä½æº«å½±éŸ¿ã€‚"
    if avg_temp > opt_max:
        return "åé«˜", "âš ï¸", "æ°£æº«åé«˜ï¼Œå¯èƒ½å¢åŠ ç†±é€†å¢ƒé¢¨éšªï¼Œéœ€æ³¨æ„æ°´åˆ†ç®¡ç†ã€‚"
    return "é©å®œ", "âœ…", "æ°£æº«æ¢ä»¶é©ä¸­ï¼Œæœ‰åˆ©ä½œç‰©æ­£å¸¸ç”Ÿé•·ã€‚"


# ===============================
# Sidebar
# ===============================
st.sidebar.header("ğŸ”§ æƒ…å¢ƒè¨­å®š")

region = st.sidebar.selectbox(
    "ğŸ“ åˆ†æåœ°å€ï¼ˆç¤ºç¯„ï¼‰",
    ["å…¨å°"],
    key="region_select",
)

crop = st.sidebar.selectbox(
    "ğŸŒ¾ ä½œç‰©é¡å‹",
    ["æ°´ç¨»", "ç‰ç±³", "é«˜éº—èœ", "ç•ªèŒ„"],
    key="crop_select",
)

st.sidebar.markdown("### ğŸ“… é å ±æœŸé–“")
st.sidebar.info("ä»¥ **ä»Šæ—¥èµ·ç®—ä¹‹æœªä¾† 7 å¤©æ°£è±¡é å ±** é€²è¡Œè§£è®€")

st.sidebar.markdown("---")
st.sidebar.info(
    "ğŸ“Œ æœ¬ç³»çµ±ç‚ºã€Œé å ±è§£è®€å‹ Demoã€\n\n"
    "- ä½¿ç”¨ CWA ä¸€é€±æ°£è±¡é å ±è³‡æ–™\n"
    "- æä¾›ä½œç‰©ç”Ÿé•·æ¢ä»¶èˆ‡é¢¨éšªåˆ¤æ–·\n"
    "- éæ­·å²å›æº¯åˆ†æ"
)

st.sidebar.markdown("---")
st.sidebar.subheader("â˜ï¸ Cloud è³‡æ–™")
st.sidebar.caption("Cloud ä¸æœƒæœ‰ä½ æœ¬æ©Ÿçš„ weather_dataï¼Œå› æ­¤éœ€åœ¨é›²ç«¯åŸ·è¡Œ crawler.py ç”¢ç”Ÿè³‡æ–™ã€‚")


# ===============================
# Main
# ===============================
st.title("ğŸŒ¤ï¸ ä¸€é€±è¾²æ¥­æ°£è±¡é å ± + è¾²æ¥­ç©æº«åˆ†æ")

data, latest_fname = load_latest_json()

if data is None:
    st.warning("âš ï¸ å°šæœªè¼‰å…¥æ°£è±¡é å ±è³‡æ–™ï¼ˆCloud æ²’æœ‰æœ¬æ©Ÿæª”æ¡ˆï¼‰")

    col1, col2 = st.columns([1, 3])
    with col1:
        run = st.button("ğŸ”„ ç«‹å³æŠ“å–æœ€æ–°é å ±", key="run_crawler_btn")
    with col2:
        st.caption("æŒ‰ä¸‹å¾Œæœƒåœ¨é›²ç«¯åŸ·è¡Œ crawler.pyï¼Œä¸¦ç”¢ç”Ÿ weather_data/*.jsonï¼ˆè‹¥éœ€è¦ API KEYï¼Œè«‹åœ¨ Cloud Secrets è¨­å®šï¼‰ã€‚")

    with st.expander("ğŸ§© å¦‚æœæŠ“ä¸åˆ°è³‡æ–™æˆ‘è©²æ€éº¼åšï¼Ÿ"):
        st.markdown(
            """
1) ç¢ºèª repo å…§æœ‰ `crawler.py`  
2) `requirements.txt` è‡³å°‘åŒ…å«ï¼š`streamlit`, `requests`, `pandas`  
3) è‹¥ CWA API éœ€è¦é‡‘é‘°ï¼Œè«‹åˆ° Streamlit Cloud â†’ **Manage app** â†’ **Settings â†’ Secrets** åŠ å…¥ï¼š
```toml
CWA_API_KEY="ä½ çš„key"
